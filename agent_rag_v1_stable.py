import os
import sys
import redis
from typing import List
from typing_extensions import TypedDict

# --- 1. Imports (å…¼å®¹æ€§ä¿®å¤ç‰ˆ) ---
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings 
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.graph import END, StateGraph
from langchain_community.cache import RedisCache

# ğŸ”¥ ä¿®å¤ 1ï¼šè‡ªåŠ¨æ£€æµ‹ LangChain ç‰ˆæœ¬
try:
    from langchain.globals import set_llm_cache
except ImportError:
    import langchain
    def set_llm_cache(cache):
        langchain.llm_cache = cache
    print("âš ï¸ [System] Detected older LangChain version. Using compatibility mode.")

# ==========================================
# ğŸ‘‡ Configuration Area ğŸ‘‡
# ==========================================

# 1. DeepSeek API Key
os.environ["OPENAI_API_KEY"] = "sk-abf97993407943e698adda0bdeabddb8"

# 2. DeepSeek Base URL
BASE_URL = "https://api.deepseek.com"

# 3. Model Name
MODEL_NAME = "deepseek-chat"

# ==========================================
# ğŸš€ Enterprise Upgrade: Redis Caching Layer (ä¸‡èƒ½ä¿®å¤ç‰ˆ)
# ==========================================
print("ğŸ”Œ [System] Connecting to Redis Cache...")
try:
    # 1. å…ˆå»ºç«‹ Redis è¿æ¥
    client = redis.Redis(host="localhost", port=6379, db=0)
    
    # 2. å°è¯•å¤šç§å‚æ•°å†™æ³•ï¼Œç›´åˆ°æˆåŠŸä¸ºæ­¢
    try:
        # å†™æ³• A: æ–°ç‰ˆæ ‡å‡†
        set_llm_cache(RedisCache(redis_client=client))
    except TypeError:
        try:
            # å†™æ³• B: ä½ çš„ç‰ˆæœ¬æç¤ºçš„å‚æ•° (redis_)
            set_llm_cache(RedisCache(redis_=client))
        except TypeError:
            # å†™æ³• C: å¦ä¸€ç§æ—§ç‰ˆå†™æ³• (redis_url)
            set_llm_cache(RedisCache(redis_url="redis://localhost:6379/0"))
            
    print("âœ… [Cache] Redis connected! Repeated queries will have near-zero latency.")
except Exception as e:
    print(f"âš ï¸ [Cache] Redis connection failed. Running without cache. Error: {e}")

# ==========================================

# --- 2. Data Processing (ETL Pipeline) ---
print("âš™ï¸ [ETL] Loading and cleaning data...")

if not os.path.exists("manual_parsed.md"):
    print("âŒ Error: 'manual_parsed.md' not found! Please check the file path.")
    sys.exit(1)

loader = UnstructuredMarkdownLoader("manual_parsed.md")
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""]
)
splits = text_splitter.split_documents(docs)
print(f"âœ… [ETL] Document split into {len(splits)} semantic chunks")

# --- 3. Vector Database (Vector Store) ---
print("ğŸ’¾ [DB] Loading local vector database...")

# ğŸ”¥ Local Embeddings (Privacy-first)
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embedding_model,
    collection_name="manual_rag",
    persist_directory="./chroma_db"
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# --- 4. Define Graph State ---
class GraphState(TypedDict):
    question: str
    context: str
    answer: str

# --- 5. Define Nodes ---
def retrieve(state: GraphState):
    """Retrieval Node"""
    print(f"ğŸ” [Retriever] Retrieving info for: '{state['question']}'...")
    documents = retriever.invoke(state["question"])
    context = "\n\n".join([doc.page_content for doc in documents])
    return {"context": context}

def generate(state: GraphState):
    """Generation Node"""
    print("ğŸ¤– [Generator] LLM is thinking (checking cache first)...")
    
    prompt = f"""You are a professional technical consultant. Answer the user's question based ONLY on the context provided below.
    
    If the answer is not in the context, simply say "I don't know based on the provided documents." Do not make up information.
    
    Context:
    {state['context']}
    
    Question:
    {state['question']}
    """
    
    llm = ChatOpenAI(
        model=MODEL_NAME, 
        temperature=0, 
        base_url=BASE_URL,
        api_key=os.environ["OPENAI_API_KEY"]
    )
    response = llm.invoke(prompt)
    return {"answer": response.content}

# --- 6. Build Workflow (LangGraph) ---
print("ğŸ”— [Graph] Compiling LangGraph workflow...")
workflow = StateGraph(GraphState)

workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate)

workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)

app = workflow.compile()

# --- 7. Execution ---
if __name__ == "__main__":
    question = "What are the course chapters listed in this manual? Please summarize them."
    
    print(f"\nğŸš€ Starting Task: {question}")
    
    try:
        result = app.invoke({"question": question})
        print("\n" + "="*30 + " FINAL ANSWER " + "="*30)
        print(result["answer"])
        print("="*74)
    except Exception as e:
        print(f"\nâŒ Execution Error: {e}")
