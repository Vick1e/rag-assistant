import streamlit as st
import os
import sys
import redis
import base64
from typing_extensions import TypedDict

# --- LangChain Imports ---
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings 
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.graph import END, StateGraph
from langchain_community.cache import RedisCache

# ğŸ”¥ 1. æ··åˆæ£€ç´¢æ¨¡å—æ£€æµ‹
HYBRID_SEARCH_AVAILABLE = False
try:
    from langchain.retrievers import EnsembleRetriever
    from langchain_community.retrievers import BM25Retriever
    import rank_bm25
    HYBRID_SEARCH_AVAILABLE = True
except ImportError:
    pass

# ğŸ”¥ 2. LangChain ç‰ˆæœ¬é€‚é…
try:
    from langchain.globals import set_llm_cache
except ImportError:
    import langchain
    def set_llm_cache(cache):
        langchain.llm_cache = cache

# ==========================================
# ğŸ¨ UI & æ ·å¼é…ç½® (ä¿®æ”¹äº†æ ‡é¢˜)
# ==========================================
st.set_page_config(
    page_title="Knowledge Q&A Bot",  # ğŸŸ¢ æ”¹åŠ¨ç‚¹ 1ï¼šæµè§ˆå™¨æ ‡ç­¾é¡µæ ‡é¢˜
    page_icon="ğŸ¤–", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 1. å®šä¹‰ç”Ÿæˆå¤´åƒçš„å‡½æ•° ---
def get_icon_base64(color_hex):
    """ç”Ÿæˆçº¯è‰² SVG å¤´åƒçš„ Base64 å­—ç¬¦ä¸²"""
    svg = f"""<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><circle cx="50" cy="50" r="50" fill="{color_hex}" /></svg>"""
    return base64.b64encode(svg.encode("utf-8")).decode("utf-8")

# --- 2. ç”Ÿæˆå…·ä½“çš„å¤´åƒæ•°æ® ---
# å¤©ç©ºè“ (User)
USER_COLOR = "#87CEEB"
user_b64 = get_icon_base64(USER_COLOR)
USER_AVATAR = f"data:image/svg+xml;base64,{user_b64}"

# æ·¡ç²‰è‰² (Bot)
BOT_COLOR = "#FFB6C1"
bot_b64 = get_icon_base64(BOT_COLOR)
BOT_AVATAR = f"data:image/svg+xml;base64,{bot_b64}"

# --- 3. åŠ¨æ€æ³¨å…¥ CSS (å·¦å³å¯¹è¯æ°”æ³¡) ---
st.markdown(f"""
<style>
    /* éšè—é¡¶éƒ¨èœå• */
    #MainMenu {{visibility: hidden;}}
    footer {{visibility: hidden;}}
    
    /* èŠå¤©æ°”æ³¡åŸºç¡€æ ·å¼ */
    [data-testid="stChatMessage"] {{
        padding: 1rem;
        border-radius: 15px;
        margin-bottom: 1rem;
        display: flex;
        gap: 1rem;
    }}
    
    /* ğŸ”¥ ç”¨æˆ·æ¶ˆæ¯ (é å³ï¼Œè“è‰²èƒŒæ™¯) */
    [data-testid="stChatMessage"]:has(img[src="{USER_AVATAR}"]) {{
        flex-direction: row-reverse;
        background-color: rgba(135, 206, 235, 0.15);
        border: 1px solid {USER_COLOR};
        text-align: right;
    }}
    
    /* ğŸ”¥ æœºå™¨äººæ¶ˆæ¯ (é å·¦ï¼Œç²‰è‰²èƒŒæ™¯) */
    [data-testid="stChatMessage"]:has(img[src="{BOT_AVATAR}"]) {{
        background-color: rgba(255, 182, 193, 0.15);
        border: 1px solid {BOT_COLOR};
    }}
    
    /* è°ƒæ•´å¤´åƒå¤§å° */
    [data-testid="stChatMessageAvatar"] img {{
        width: 45px;
        height: 45px;
    }}
</style>
""", unsafe_allow_html=True)

# é…ç½® API Key
os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY_HERE"
BASE_URL = "https://api.deepseek.com"
MODEL_NAME = "deepseek-chat"

# ==========================================
# ğŸš€ æ ¸å¿ƒç³»ç»Ÿåˆå§‹åŒ–
# ==========================================
@st.cache_resource
def initialize_system():
    status = {"redis": False, "hybrid": False}
    
    try:
        client = redis.Redis(host="localhost", port=6379, db=0)
        try:
            set_llm_cache(RedisCache(redis_client=client))
        except TypeError:
            set_llm_cache(RedisCache(redis_=client))  
        status["redis"] = True
    except Exception:
        pass 

    if not os.path.exists("manual_parsed.md"):
        st.error("âŒ Critical Error: 'manual_parsed.md' file missing!")
        st.stop()

    loader = UnstructuredMarkdownLoader("manual_parsed.md")
    docs = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=200, separators=["\n\n", "\n", " ", ""]
    )
    splits = text_splitter.split_documents(docs)
    
    embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embedding_model,
        collection_name="manual_rag_web", 
        persist_directory="./chroma_db_web"
    )
    
    # æ£€ç´¢å™¨è®¾ç½® (k=6)
    base_retriever = vectorstore.as_retriever(search_kwargs={"k": 6})
    
    if HYBRID_SEARCH_AVAILABLE:
        try:
            bm25_retriever = BM25Retriever.from_documents(splits)
            bm25_retriever.k = 6
            ensemble_retriever = EnsembleRetriever(
                retrievers=[base_retriever, bm25_retriever],
                weights=[0.5, 0.5]
            )
            retriever = ensemble_retriever
            status["hybrid"] = True
        except Exception:
            retriever = base_retriever
    else:
        retriever = base_retriever
        
    return retriever, status

with st.spinner('ğŸš€ Booting Knowledge Q&A Bot...'):
    retriever, system_status = initialize_system()

# ==========================================
# ğŸ¨ ä¾§è¾¹æ 
# ==========================================
with st.sidebar:
    st.header("âš™ï¸ System Status")
    
    if system_status["redis"]:
        st.success("âœ… Redis: On")
    else:
        st.warning("âš ï¸ Redis: Off")
    
    if system_status["hybrid"]:
        st.success("âœ… Hybrid Search: On")
    else:
        st.info("â„¹ï¸ Vector Search Only")
    
    st.divider()
    st.caption("Knowledge Source: manual_parsed.md")

# ==========================================
# ğŸ§  Agent é€»è¾‘
# ==========================================
class GraphState(TypedDict):
    question: str
    context: str
    answer: str

def retrieve(state: GraphState):
    documents = retriever.invoke(state["question"])
    context = "\n\n".join([doc.page_content for doc in documents])
    return {"context": context}

def generate(state: GraphState):
    prompt = f"""You are a helpful knowledge assistant. Answer based ONLY on the context provided.
    
    Context:
    {state['context']}
    
    Question:
    {state['question']}
    """
    llm = ChatOpenAI(model=MODEL_NAME, temperature=0, base_url=BASE_URL, api_key=os.environ["OPENAI_API_KEY"])
    response = llm.invoke(prompt)
    return {"answer": response.content}

workflow = StateGraph(GraphState)
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate)
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)
app = workflow.compile()

# ==========================================
# ğŸ’¬ èŠå¤©ç•Œé¢ (UI)
# ==========================================
st.title("ğŸ¤– Knowledge Q&A Bot") # ğŸŸ¢ æ”¹åŠ¨ç‚¹ 2ï¼šé¡µé¢ä¸»æ ‡é¢˜
st.caption("Ask specific questions about the knowledge base.")

if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    icon = USER_AVATAR if message["role"] == "user" else BOT_AVATAR
    with st.chat_message(message["role"], avatar=icon):
        st.markdown(message["content"])

# å¤„ç†è¾“å…¥
if prompt := st.chat_input("Ask a question..."):
    # 1. ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar=USER_AVATAR):
        st.markdown(prompt)

    # 2. AI æ¶ˆæ¯
    with st.chat_message("assistant", avatar=BOT_AVATAR):
        message_placeholder = st.empty()
        full_response = ""
        
        with st.spinner("ğŸ§  AI is thinking..."):
            try:
                result = app.invoke({"question": prompt})
                full_response = result["answer"]
                message_placeholder.markdown(full_response)
            except Exception as e:
                st.error(f"âŒ Error: {e}")
                full_response = "Sorry, I encountered an error."
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})